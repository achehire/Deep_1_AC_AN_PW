{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PATH TO DATA. CHANGE IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r'D:\\Scolaire\\UdeM\\IFT_6135\\Assignment1\\data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data generator (for Keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_image(ID, which):\n",
    "    ids = int(ID)\n",
    "    dir_path = os.path.join(data_path, which + 'set')\n",
    "    if which=='train':\n",
    "        if ids > 9999:\n",
    "            ids = ids - 10000\n",
    "            clas = 'Dog'\n",
    "        else:\n",
    "            clas = 'Cat'\n",
    "        with Image.open(os.path.join(dir_path, clas, str(ids)+'.'+clas+'.jpg')) as im:\n",
    "            im_data = np.asarray(im)\n",
    "    elif which=='test':\n",
    "        with Image.open(os.path.join(dir_path, 'test', str(ids)+'.jpg')) as im:\n",
    "            im_data = np.asarray(im)\n",
    "    else:\n",
    "        raise AssertionError(\"Wrong value in fetch image\")\n",
    "        \n",
    "    s = im_data.shape\n",
    "    if len(s) == 2:  # ie gray scale\n",
    "        new_array = np.empty((*s, 3), dtype='int')\n",
    "        for i in range(s[0]):\n",
    "            for j in range(s[1]):\n",
    "                new_array[i,j] = np.repeat(np.array([im_data[i,j]]), 3)\n",
    "        return new_array\n",
    "    else:\n",
    "        return im_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGeneratorTrain(keras.utils.Sequence):\n",
    "    \n",
    "    def __init__(self, list_IDs, labels, batch_size=32, dim=(64,64), n_channels=3,\n",
    "                 n_classes=2, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "          np.random.shuffle(self.indexes)\n",
    "            \n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "        y = np.empty((self.batch_size), dtype=int)\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Store sample\n",
    "            X[i,] = fetch_image(ID, 'train')\n",
    "\n",
    "            # Store class\n",
    "            y[i] = self.labels[ID]\n",
    "\n",
    "        if self.n_classes > 2:  \n",
    "            return X, keras.utils.to_categorical(y, num_classes=self.n_classes)\n",
    "        else:\n",
    "            return X, y\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "params = {'dim': (64,64),\n",
    "          'batch_size': 64,\n",
    "          'n_classes': 2,\n",
    "          'n_channels': 3,\n",
    "          'shuffle': True}\n",
    "val_size = 0.2\n",
    "\n",
    "# Datasets\n",
    "total_ids = np.hstack((np.arange(1, 10000), np.arange(10001, 20000)))\n",
    "number_ids = len(total_ids)\n",
    "labels = dict()\n",
    "for ID in total_ids:\n",
    "    labels[str(ID)] = int(ID>10000)\n",
    "partition = dict()\n",
    "np.random.seed(0)\n",
    "np.random.shuffle(total_ids)\n",
    "partition['train'] = [str(x) for x in total_ids[:int(number_ids*(1-val_size))]]\n",
    "partition['validation'] = [str(x) for x in total_ids[int(number_ids*(1-val_size)):]]\n",
    "\n",
    "# Generators\n",
    "training_generator = DataGeneratorTrain(partition['train'], labels, **params)\n",
    "validation_generator = DataGeneratorTrain(partition['validation'], labels, **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN1:\n",
    "    \n",
    "    def __init__(self):\n",
    "        np.random.seed(0)\n",
    "        tf.set_random_seed(0)\n",
    "        self.model = keras.models.Sequential()\n",
    "        self.history = None\n",
    "        \n",
    "        # Layer 1\n",
    "        self.model.add(Conv2D(32, kernel_size=3, strides=1, padding='same', use_bias=True,\n",
    "                              activation='relu', input_shape=(64,64,3), data_format='channels_last'))\n",
    "        self.model.add(MaxPooling2D(2, padding='valid', data_format='channels_last'))\n",
    "        \n",
    "        # Layer 2\n",
    "        self.model.add(Conv2D(64, kernel_size=3, strides=1, padding='same', use_bias=True,\n",
    "                              activation='relu', input_shape=(32,32,64), data_format='channels_last'))\n",
    "        self.model.add(MaxPooling2D(2, padding='valid', data_format='channels_last'))\n",
    "        \n",
    "        # Layer 3\n",
    "        self.model.add(Conv2D(128, kernel_size=3, strides=1, padding='same', use_bias=True,\n",
    "                              activation='relu', input_shape=(16,16,128), data_format='channels_last'))\n",
    "        self.model.add(MaxPooling2D(2, padding='valid', data_format='channels_last'))\n",
    "        \n",
    "        # Layer 4\n",
    "        self.model.add(Flatten())\n",
    "        self.model.add(Dense(512, use_bias=True, activation='relu'))\n",
    "        \n",
    "        # Layer 5\n",
    "        self.model.add(Dense(1, use_bias=True, activation='sigmoid'))\n",
    "        \n",
    "        sgd = keras.optimizers.SGD(lr=0.01)\n",
    "        self.model.compile(optimizer=sgd, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        self.model.summary()\n",
    "    \n",
    "    def train(self, ep, early=10):\n",
    "        earlystop = EarlyStopping(patience=early)\n",
    "        learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n",
    "                                                    patience=5, \n",
    "                                                    verbose=1, \n",
    "                                                    factor=0.5, \n",
    "                                                    min_lr=0.00001)\n",
    "        callbacks = [earlystop, learning_rate_reduction]\n",
    "        \n",
    "        self.history = self.model.fit_generator(generator=training_generator,\n",
    "                                                validation_data=validation_generator, \n",
    "                                                epochs=ep, callbacks=callbacks)\n",
    "        # use_multiprocessing=True, workers=6,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_28 (Conv2D)           (None, 64, 64, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_28 (MaxPooling (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 32, 32, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_29 (MaxPooling (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_30 (Conv2D)           (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_30 (MaxPooling (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 512)               4194816   \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 4,288,577\n",
      "Trainable params: 4,288,577\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mod1 = CNN1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "249/249 [==============================] - 131s 525ms/step - loss: 7.9205 - acc: 0.5021 - val_loss: 8.0957 - val_acc: 0.4922\n",
      "Epoch 2/15\n",
      "249/249 [==============================] - 132s 528ms/step - loss: 7.9372 - acc: 0.5021 - val_loss: 8.1158 - val_acc: 0.4909\n",
      "Epoch 3/15\n",
      "249/249 [==============================] - 133s 533ms/step - loss: 7.9352 - acc: 0.5023 - val_loss: 8.1038 - val_acc: 0.4917\n",
      "Epoch 4/15\n",
      "249/249 [==============================] - 133s 533ms/step - loss: 7.9452 - acc: 0.5016 - val_loss: 8.0998 - val_acc: 0.4919\n",
      "Epoch 5/15\n",
      "249/249 [==============================] - 131s 526ms/step - loss: 7.9342 - acc: 0.5023 - val_loss: 8.1118 - val_acc: 0.4912\n",
      "Epoch 6/15\n",
      "249/249 [==============================] - 138s 554ms/step - loss: 7.9422 - acc: 0.5018 - val_loss: 8.1118 - val_acc: 0.4912\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.05000000074505806.\n",
      "Epoch 7/15\n",
      "249/249 [==============================] - 131s 527ms/step - loss: 7.9352 - acc: 0.5023 - val_loss: 8.1239 - val_acc: 0.4904\n",
      "Epoch 8/15\n",
      "249/249 [==============================] - 131s 525ms/step - loss: 7.9412 - acc: 0.5019 - val_loss: 8.0917 - val_acc: 0.4924\n",
      "Epoch 9/15\n",
      "249/249 [==============================] - 134s 540ms/step - loss: 7.9372 - acc: 0.5021 - val_loss: 8.0877 - val_acc: 0.4927\n",
      "Epoch 10/15\n",
      "249/249 [==============================] - 133s 532ms/step - loss: 7.9372 - acc: 0.5021 - val_loss: 8.0998 - val_acc: 0.4919\n",
      "Epoch 11/15\n",
      "249/249 [==============================] - 140s 563ms/step - loss: 7.9342 - acc: 0.5023 - val_loss: 8.1038 - val_acc: 0.4917\n",
      "Epoch 12/15\n",
      "249/249 [==============================] - 136s 548ms/step - loss: 7.9412 - acc: 0.5019 - val_loss: 8.1078 - val_acc: 0.4914\n",
      "Epoch 13/15\n",
      "249/249 [==============================] - 138s 553ms/step - loss: 7.9302 - acc: 0.5026 - val_loss: 8.1118 - val_acc: 0.4912\n",
      "Epoch 14/15\n",
      "249/249 [==============================] - 135s 542ms/step - loss: 7.9342 - acc: 0.5023 - val_loss: 8.1239 - val_acc: 0.4904\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.02500000037252903.\n",
      "Epoch 15/15\n",
      "249/249 [==============================] - 132s 532ms/step - loss: 7.9292 - acc: 0.5026 - val_loss: 8.1118 - val_acc: 0.4912\n"
     ]
    }
   ],
   "source": [
    "mod1.train(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
