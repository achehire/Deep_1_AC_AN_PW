{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; font-size: 30pt; font-weight: bold; margin: 1em 0em 1em 0em\">Homework 1</div>\n",
    "\n",
    "$\\textbf{Authors}$ : Adel Nabli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "L:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from time import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data in the format of a design matrix for X\n",
    "# and one-hot encode Y\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = np.reshape(x_train, (len(x_train), 784))\n",
    "X_test = np.reshape(x_test, (len(x_test), 784))\n",
    "Y_train = OneHotEncoder().fit_transform(np.reshape(y_train, (len(y_train),1))).toarray()\n",
    "Y_test = OneHotEncoder().fit_transform(np.reshape(y_test, (len(y_test),1))).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to build a MLP with two hidden layers with $h^1$ and $h^2$ hidden units. The data is given in the form of a design matrix $X \\in \\mathbb{R}^{n \\times 784}$ and $\\forall i \\in [\\![1,n]\\!], y^i \\in \\mathbb{R}^{10}$ is the one-hot encoding of the label.\n",
    "The output layer is parametrized by a $softmax$ function $s(z)_c = \\dfrac{e^{z_c}}{\\sum_c e^{z_c}}$.\n",
    "\n",
    "* We choose the sigmoid function as a non linearity for both layers.\n",
    "We have: $\\sigma(z) = \\dfrac{1}{1+ e^{-z}}$ and $\\sigma'(z) = (1-\\sigma(z))\\sigma(z)$.\n",
    "\n",
    "\n",
    "* Thus, our NN is computing, for each example $x \\in \\mathbb{R}^{784}$:\n",
    "\n",
    "    - $A_1 = \\sigma(W_1 x + b_1)$ with $W_1 \\in \\mathbb{R}^{h_1 \\times 784}$ and $b_1 \\in \\mathbb{R}^{h_1}$\n",
    "    - $A_2 = \\sigma(W_2 A_1 + b_2)$ with $W_2 \\in \\mathbb{R}^{h_2 \\times h_1}$ and $b_2 \\in \\mathbb{R}^{h_2}$\n",
    "    - $O = s(W_3 A_2 + b_3)$ with $W_3 \\in \\mathbb{R}^{10 \\times h_2}$ and $b_3 \\in \\mathbb{R}^{10}$\n",
    "\n",
    "\n",
    "* The cross entropy loss is defined by: $l(y, O) = -\\sum_{c=1}^{10} y_c\\log(o_c)$ (recall that $y_c = \\mathbb{1}_{label=c}$) which leads to an empirical risk $\\hat{R} = \\dfrac{-1}{n} \\sum_{i=1}^n \\sum_{c=1}^{10} y_c^i\\log(o_c^i)$.\n",
    "\n",
    "* Our purpose is to learn each parameter $\\theta \\in \\{ W_1, b_1, W_2, b_2, W_3, b_3 \\}$ by **stochastic** gradient descent incrementaly: $\\theta^{t+1} = \\theta^t - \\alpha_t \\dfrac{\\partial l}{\\partial \\theta}(x_t, y_t)$  with $\\alpha_t, x_t, y_t$ being respectively the learning rate, the training example and the corresponding label at time $t$. We use the formula $\\alpha_t = \\dfrac{\\alpha_0}{1 + \\delta t}$ for the learning rate.\n",
    "\n",
    "\n",
    "* For that, using the chain-rule, we derive:\n",
    "\n",
    "    - $\\dfrac{\\partial l}{\\partial b_3} = O - y$\n",
    "    - $\\dfrac{\\partial l}{\\partial W_3} = (O - y)A_2^{T}$\n",
    "    - $\\dfrac{\\partial l}{\\partial b_2} = \\dfrac{\\partial l}{\\partial A_2}*(1-A_2)*A_2$ with $\\dfrac{\\partial l}{\\partial A_2} = W_3 ^T  \\dfrac{\\partial l}{\\partial b_3}$\n",
    "    - $\\dfrac{\\partial l}{\\partial W_2} = \\dfrac{\\partial l}{\\partial b_2 } A_1 ^T$\n",
    "    - $\\dfrac{\\partial l}{\\partial b_1} = \\dfrac{\\partial l}{\\partial A_1}*(1-A_1)*A_1$ with $\\dfrac{\\partial l}{\\partial A_1} = W_2 ^T  \\dfrac{\\partial l}{\\partial b_2}$\n",
    "    - $\\dfrac{\\partial l}{\\partial W_1} = \\dfrac{\\partial l}{\\partial b_1 } x ^T$\n",
    "\n",
    "\n",
    "* To initialize our parameters, we have three options:\n",
    "    - **Zero**: we initialize with zeros\n",
    "    - **Normal**: we initialize with $\\mathcal{N}(0,1)$\n",
    "    - **Glorot**: we initialize with $\\mathcal{U}(-d^l, d^l)$ where $d^l = \\sqrt{\\dfrac{6}{h^{l-1}+h^l}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(object):\n",
    "    \n",
    "    def __init__(self, hidden_dims=(1024,2048), initialization='glorot', learning_rate=0.001, delta=0.7):\n",
    "        \n",
    "        dims = (784, hidden_dims[0], hidden_dims[1], 10)\n",
    "        W1, b1, W2, b2, W3, b3 = self.initialize_weights(dims, initialization)\n",
    "        parameters = dict()\n",
    "        parameters['W1'] = W1\n",
    "        parameters['b1'] = b1\n",
    "        parameters['W2'] = W2\n",
    "        parameters['b2'] = b2\n",
    "        parameters['W3'] = W3\n",
    "        parameters['b3'] = b3\n",
    "        \n",
    "        cache = dict()\n",
    "        cache['x'] = None\n",
    "        cache['A1'] = None\n",
    "        cache['A2'] = None\n",
    "        cache['O'] = None\n",
    "        cache['y'] = None\n",
    "        \n",
    "        grad = dict()\n",
    "        grad['W1'] = None\n",
    "        grad['b1'] = None\n",
    "        grad['W2'] = None\n",
    "        grad['b2'] = None\n",
    "        grad['W3'] = None\n",
    "        grad['b3'] = None\n",
    "        \n",
    "        self.parameters = parameters\n",
    "        self.cache = cache\n",
    "        self.grad = grad\n",
    "        self.learning_rate = learning_rate\n",
    "        self.delta = delta\n",
    "        self.cpt = 0\n",
    "        self.n_samples = 0\n",
    "        \n",
    "    def initialize_weights(self,dims, initialization):\n",
    "        \n",
    "        # dims = (dim_input, h1, h2, dim_output)\n",
    "        \n",
    "        np.random.seed(42)\n",
    "        \n",
    "        d, h1, h2, o = dims\n",
    "        b1 = np.zeros(h1)\n",
    "        b2 = np.zeros(h2)\n",
    "        b3 = np.zeros(o)\n",
    "        \n",
    "        if initialization == 'zero':\n",
    "            \n",
    "            W1 = np.zeros((h1, d))\n",
    "            W2 = np.zeros((h2,h1))\n",
    "            W3 = np.zeros((o,h2))\n",
    "        \n",
    "        elif initialization == 'normal':\n",
    "            \n",
    "            W1 = np.random.normal(0,1,(h1,d))\n",
    "            W2 = np.random.normal(0,1,(h2,h1))\n",
    "            W3 = np.random.normal(0,1,(o,h2))\n",
    "        \n",
    "        elif initialization == 'glorot':\n",
    "            \n",
    "            d1 = np.sqrt(6/(d+h1))\n",
    "            d2 = np.sqrt(6/(h1+h2))\n",
    "            d3 = np.sqrt(6/(h2+o))\n",
    "            \n",
    "            W1 = np.random.uniform(-d1,d1, (h1,d))\n",
    "            W2 = np.random.uniform(-d2,d2, (h2,h1))\n",
    "            W3 = np.random.uniform(-d3,d3, (o,h2))\n",
    "        \n",
    "        return(W1, b1, W2, b2, W3, b3)\n",
    "\n",
    "    def forward(self,x,y):\n",
    "        \n",
    "        W1 = self.parameters['W1']\n",
    "        b1 = self.parameters['b1']\n",
    "        W2 = self.parameters['W2']\n",
    "        b2 = self.parameters['b2']\n",
    "        W3 = self.parameters['W3']\n",
    "        b3 = self.parameters['b3']\n",
    "        \n",
    "        A1 = self.activation(np.dot(W1,x)+b1)\n",
    "        A2 = self.activation(np.dot(W2,A1)+b2)\n",
    "        O = self.softmax(np.dot(W3,A2)+b3)\n",
    "        \n",
    "        self.cache['x'] = x\n",
    "        self.cache['A1'] = A1\n",
    "        self.cache['A2'] = A2\n",
    "        self.cache['O'] = O\n",
    "        self.cache['y'] = y\n",
    "\n",
    "    def activation(self,U):\n",
    "        \n",
    "        return(1/(1+np.exp(-U)))\n",
    "\n",
    "    def loss(self, O, y_true):\n",
    "        \n",
    "        return(-np.log(np.dot(O,y_true)))\n",
    "        \n",
    "    def softmax(self,U):\n",
    "        \n",
    "        return(np.exp(U - np.log(np.sum(np.exp(U), axis=0))))\n",
    "\n",
    "    def backward(self):\n",
    "        \n",
    "        x = self.cache['x']\n",
    "        A1 = self.cache['A1']\n",
    "        A2 = self.cache['A2']\n",
    "        O = self.cache['O']\n",
    "        y = self.cache['y']\n",
    "        W3 = self.parameters['W3']\n",
    "        W2 = self.parameters['W2']\n",
    "        \n",
    "        db3 = O-y\n",
    "        self.grad['b3'] = db3\n",
    "        self.grad['W3'] = np.dot(np.reshape(db3, (len(db3),1)), np.reshape(A2, (1, len(A2))))\n",
    "        dA2 = np.dot(W3.T,db3)\n",
    "        self.grad['b2'] = dA2*(1-A2)*A2\n",
    "        db2 = self.grad['b2']\n",
    "        self.grad['W2'] = np.dot(np.reshape(db2, (len(db2),1)), np.reshape(A1, (1, len(A1))))\n",
    "        dA1 = np.dot(W2.T,db2)\n",
    "        self.grad['b1'] = dA1*(1-A1)*A1\n",
    "        db1 = self.grad['b1']\n",
    "        self.grad['W1'] = np.dot(np.reshape(db1, (len(db1),1)), np.reshape(x, (1, len(x))))\n",
    "\n",
    "    def update(self):\n",
    "        \n",
    "        alpha_0 = self.learning_rate\n",
    "        delta = self.delta\n",
    "        self.cpt += 1\n",
    "        \n",
    "        alpha = alpha_0/(1+delta*self.cpt)\n",
    "        \n",
    "        self.parameters['W1'] -= alpha*self.grad['W1']\n",
    "        self.parameters['b1'] -= alpha*self.grad['b1']\n",
    "        self.parameters['W2'] -= alpha*self.grad['W2']\n",
    "        self.parameters['b2'] -= alpha*self.grad['b2']\n",
    "        self.parameters['W3'] -= alpha*self.grad['W3']\n",
    "        self.parameters['b3'] -= alpha*self.grad['b3']\n",
    "\n",
    "    def train(self, X_train, Y_train, X_test, Y_test, n_epoch):\n",
    "        \n",
    "        n = len(X_train)\n",
    "        self.n_samples = n\n",
    "        ids = np.arange(n)\n",
    "        averaged_loss_train = []\n",
    "        averaged_loss_test = []\n",
    "        accuracy_train = []\n",
    "        accuracy_test = []\n",
    "        \n",
    "        for epoch in range(n_epoch):\n",
    "            \n",
    "            t0 = time()\n",
    "            \n",
    "            print('Epoch : %s' %epoch, '\\n')\n",
    "            \n",
    "            np.random.shuffle(ids)\n",
    "            risk = 0\n",
    "            \n",
    "            for i in tqdm(range(n)):\n",
    "                \n",
    "                x = X_train[ids[i]]\n",
    "                y = Y_train[ids[i]]\n",
    "                \n",
    "                self.forward(x,y)\n",
    "                self.backward()\n",
    "                self.update()\n",
    "                \n",
    "                risk += self.loss(self.cache['O'], y)\n",
    "            \n",
    "            risk = risk/n\n",
    "            averaged_loss_train.append(risk)\n",
    "            Y_predict_test = self.predict(X_test.T)\n",
    "            averaged_loss_test.append(np.mean(np.diag(self.loss(Y_predict_test.T, Y_test.T))))\n",
    "            \n",
    "            Y_predict_train = self.predict(X_train.T)\n",
    "            y_predict_train = np.argmax(Y_predict_train, axis=0)\n",
    "            y_true_train = np.where(Y_train==1)[1]\n",
    "            y_predict_test = np.argmax(Y_predict_test, axis=0)\n",
    "            y_true_test = np.where(Y_test==1)[1]\n",
    "            accuracy_train.append(accuracy_score(y_true_train, y_predict_train))\n",
    "            accuracy_test.append(accuracy_score(y_true_test, y_predict_test))\n",
    "            \n",
    "            t1 = time() - t0\n",
    "            print('Training loss = %s' %risk, '\\n')\n",
    "            print('Time taken = %s' %t1, ' seconds' '\\n')\n",
    "            print('========================', '\\n')\n",
    "        \n",
    "        fig = plt.figure(figsize=(16,8))\n",
    "        ax1 = fig.add_subplot(221)\n",
    "        ax1.plot(np.arange(n_epoch), averaged_loss_train, label='training loss')\n",
    "        ax1.plot(np.arange(n_epoch), averaged_loss_test, label='test loss')\n",
    "        ax1.set_title('Evolution of the averaged loss during training')\n",
    "        ax1.legend()\n",
    "        ax2 = fig.add_subplot(222)\n",
    "        ax2.plot(np.arange(n_epoch), accuracy_train, label='accuracy train')\n",
    "        ax2.plot(np.arange(n_epoch), accuracy_test, label='accuracy test')\n",
    "        ax2.legend()\n",
    "        ax2.set_title('Evolution of the accuracy during the training')\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "    def predict(self, x):\n",
    "        \n",
    "        W1 = self.parameters['W1']\n",
    "        b1 = self.parameters['b1']\n",
    "        W2 = self.parameters['W2']\n",
    "        b2 = self.parameters['b2']\n",
    "        W3 = self.parameters['W3']\n",
    "        b3 = self.parameters['b3']\n",
    "        \n",
    "        A1 = self.activation(np.dot(W1,x)+np.reshape(b1, (len(b1),1)))\n",
    "        A2 = self.activation(np.dot(W2,A1)+np.reshape(b2, (len(b2),1)))\n",
    "        O = self.softmax(np.dot(W3,A2)+np.reshape(b3, (len(b3),1)))\n",
    "        \n",
    "        return(O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP = NN(hidden_dims=(400,700), initialization='glorot', learning_rate=0.003, delta=2e-5)\n",
    "MLP.train(X_train, Y_train, X_test, Y_test, 15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
